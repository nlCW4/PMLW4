##Introduction
For this project, the steps I will follow are those discribed during week 1.This is a simple approach and I'd like to give it a first try anyway.

##Data
As described in the video, the second step is to tune our dataset.First, I loaded the dataset. The given results of loading the training and testing dataset are the following (dim()):

    training = [1] 19622   160
    testing = [1]  20 160

I then removed all NAs and NZVs, as we did in the video.Here is the result we obtain after removing the NZVs (dim()):

    NZV <- nearZeroVar(trainRaw, saveMetrics = TRUE)
    head(NZV, 20)
    training01 <- trainRaw[, !NZV$nzv]
    testing01 <- testRaw[, !NZV$nzv]
    dim(training01)
    dim(testing01)
    rm(trainRaw)
    rm(testRaw)
    rm(NZV)
    
**Results:**

    training = [1] 19622   100
    testing = [1]  20 100

We can observe that a certain amount of data has already been removed form our dataset.
Now, if we remove the NAs datum, here is our result (dim()):

    cond <- (colSums(is.na(training)) == 0)
    training <- training[, cond]
    testing <- testing[, cond]
    rm(cond)

**Resulst:**

    training = [1] 19622    59
    testing = [1] 20 59

We observe that our dataset is now half smaller than it was at the beginning.
I will now consider that the dataset is ready for training our algorithm.
I chose a partinionning of 60% training and 40% validation, as recommended in the videos (medium-sized sample).

##Features
In this project, I didn't use automated features selection proposed by Caret.
Instead, I simply went directly to partitionning the data.

##Cross-validation
The reason for this is to have a prepared validation dataset for later cross-validation.
Here is the dataset we obtained (dim()):

    inTrain <- createDataPartition(training$classe, p = 0.70, list = FALSE)
    validation <- training[-inTrain, ]
    training <- training[inTrain, ]
    rm(inTrain)
    
**Results:**

    training = [1] 11776    59
    testing = [1] 20 59
    validation = [1] 7846   59


##Algorithms
I chose the Random Forest algorithm, as it is the one recommended in the lessons as the best for accuracy on small-sized samples of data. I used a 2 fold cross validation at first we the given result:

    accuracy = 0.9998725 
    out-of-sample error = 0.0001274535

I wasn't very happy with that accuracy, so I decided to try it all again and tweek the parameters with a 4 fold cross-validation.I had the following result:

    modelRF <- train(classe ~ ., data = training, method = "rf", trControl = trainControl(method = "cv", 4), ntree = 250)
    modelRF

**Results:**

    accuracy = 0.9998725 
    out-of-sample error = 0.0001274535

I then decided that the results are actually very good and that I will keep the Random Forest method.

    modelRF <- train(classe ~ ., data = training, method = "rf", trControl = trainControl(method = "cv", 6), ntree = 250)
    RF <- predict(modelRF, validation)
    confusionMatrix(validation$classe, RF)
    accuracy <- postResample(RF, validation$classe)
    ose <- 1 - as.numeric(confusionMatrix(validation$classe, RF)$overall[1])
    rm(RF)

**Results**
Here's the result when I applied the prediction on the testing dataset (after 6 fold cross-validation):

    [1] A A A A A A A A A A A A A A A A A A A A
    Levels: A B C D E

##Retry
I wasn't very satisfied with the result, so I decided to take another chance.
After a little help, I did everything from scratch, removed also the columns containing X, name or timestamp that weren't very useful for accelerometer data measurement, and I chose a 5 fold cross-validation.
It is likely that my mistake was to skip the features preparation step.
Here's the new results I obtained on the testing dataset:

     [1] B A B A A E D B A A B C B A E E A B B B
    Levels: A B C D E

Which looks much better, I will try this model on the prediction quizz.
